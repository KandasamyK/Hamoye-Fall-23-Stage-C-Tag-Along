{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "092ea173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Loading the dataset\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\kandh\\OneDrive\\Desktop\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "# To Convert 'TotalCharges' to numeric and fill missing values with 0\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(0, inplace=True)\n",
    "\n",
    "# To Convert 'Churn' to binary values\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# To Split the data into an 80-20 train-test split with a random state of 1\n",
    "X = df.drop(['Churn'], axis=1)  \n",
    "y = df['Churn']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# To Select features\n",
    "categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
    "                       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "                       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# To Create dataframes with only selected features\n",
    "X_train_categorical = X_train[categorical_features]\n",
    "X_test_categorical = X_test[categorical_features]\n",
    "\n",
    "X_train_numerical = X_train[numerical_features]\n",
    "X_test_numerical = X_test[numerical_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d386a4",
   "metadata": {},
   "source": [
    "Feature Scaling for Numerical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e35c2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_scaled = scaler.fit_transform(X_train_numerical)\n",
    "X_test_numerical_scaled = scaler.transform(X_test_numerical)\n",
    "\n",
    "# To Convert the scaled arrays back to dataframes with column names\n",
    "X_train_numerical_scaled_df = pd.DataFrame(X_train_numerical_scaled, columns=numerical_features)\n",
    "X_test_numerical_scaled_df = pd.DataFrame(X_test_numerical_scaled, columns=numerical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f10520d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kandh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for Categorical Features:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_train_categorical_encoded = encoder.fit_transform(X_train_categorical)\n",
    "X_test_categorical_encoded = encoder.transform(X_test_categorical)\n",
    "\n",
    "# Get the column names for one-hot encoded features\n",
    "categorical_feature_names = encoder.get_feature_names(input_features=categorical_features)\n",
    "\n",
    "# Convert the encoded arrays back to dataframes with column names\n",
    "X_train_categorical_encoded_df = pd.DataFrame(X_train_categorical_encoded, columns=categorical_feature_names)\n",
    "X_test_categorical_encoded_df = pd.DataFrame(X_test_categorical_encoded, columns=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279ff9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Combine Scaled Numerical and One-Hot Encoded Categorical Features:\n",
    "\n",
    "X_train_combined = pd.concat([X_train_numerical_scaled_df, X_train_categorical_encoded_df], axis=1)\n",
    "X_test_combined = pd.concat([X_test_numerical_scaled_df, X_test_categorical_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3822cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Train Machine Learning Models:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# To Set random_state to ensure reproducibility\n",
    "random_state = 1\n",
    "\n",
    "# To Train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=random_state)\n",
    "rf_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# To Train XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(random_state=random_state)\n",
    "xgb_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# To Train LightGBM Classifier\n",
    "lgbm_classifier = LGBMClassifier(random_state=random_state)\n",
    "lgbm_classifier.fit(X_train_combined, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6450ff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 0.7913413768630234\n",
      "Classification Report for Random Forest Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86      1061\n",
      "           1       0.58      0.53      0.56       348\n",
      "\n",
      "    accuracy                           0.79      1409\n",
      "   macro avg       0.72      0.71      0.71      1409\n",
      "weighted avg       0.79      0.79      0.79      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models on the Test Set:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# To Evaluate Random Forest model\n",
    "rf_predictions = rf_classifier.predict(X_test_combined)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_classification_report = classification_report(y_test, rf_predictions)\n",
    "\n",
    "print(f\"Random Forest Model Accuracy: {rf_accuracy}\")\n",
    "print(\"Classification Report for Random Forest Model:\\n\", rf_classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84407eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# To Define and train the ExtraTreesClassifier model\n",
    "et_classifier = ExtraTreesClassifier(random_state=1)\n",
    "et_classifier.fit(X_train_combined, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde0d594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Most Important Features:\n",
      "        Feature  Importance\n",
      "2  TotalCharges    0.126948\n",
      "0        tenure    0.117973\n"
     ]
    }
   ],
   "source": [
    "# Assuming that we have already trained the ExtraTreesClassifier model et_classifier\n",
    "\n",
    "# To Get feature importances\n",
    "feature_importances = et_classifier.feature_importances_\n",
    "\n",
    "# To Create a DataFrame to associate feature names with their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train_combined.columns, 'Importance': feature_importances})\n",
    "\n",
    "# To Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# To Get the two most important features\n",
    "top_2_features = feature_importance_df.head(2)\n",
    "\n",
    "# To Display the two most important features\n",
    "print(\"Two Most Important Features:\")\n",
    "print(top_2_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c22560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the New Optimal ExtraTreesClassifier Model: 0.7828246983676366\n",
      "Accuracy of the Initial ExtraTreesClassifier Model (No Hyperparameter Tuning): 0.7672107877927609\n",
      "The accuracy of the new optimal model is higher than the initial model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To Define the hyperparameters obtained from RandomizedSearchCV\n",
    "new_hyperparameters = {\n",
    "    'n_estimators': 300, \n",
    "    'max_depth': 15,     \n",
    "    'min_samples_split': 2,  \n",
    "    'min_samples_leaf': 1,   \n",
    "    'max_features': 'auto',  \n",
    "    'random_state': 1\n",
    "}\n",
    "\n",
    "# To Create and train the new ExtraTreesClassifier model with hyperparameters\n",
    "new_et_classifier = ExtraTreesClassifier(**new_hyperparameters)\n",
    "new_et_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# To  Make predictions on the test set for the new model\n",
    "new_et_predictions = new_et_classifier.predict(X_test_combined)\n",
    "\n",
    "# To Calculate the accuracy of the new model\n",
    "new_et_accuracy = accuracy_score(y_test, new_et_predictions)\n",
    "\n",
    "# To Calculate the accuracy of the initial model\n",
    "initial_et_predictions = et_classifier.predict(X_test_combined) \n",
    "initial_et_accuracy = accuracy_score(y_test, initial_et_predictions)\n",
    "\n",
    "# To Print the accuracy comparison\n",
    "print(\"Accuracy of the New Optimal ExtraTreesClassifier Model:\", new_et_accuracy)\n",
    "print(\"Accuracy of the Initial ExtraTreesClassifier Model (No Hyperparameter Tuning):\", initial_et_accuracy)\n",
    "\n",
    "# To Compare the accuracies\n",
    "if new_et_accuracy > initial_et_accuracy:\n",
    "    print(\"The accuracy of the new optimal model is higher than the initial model.\")\n",
    "elif new_et_accuracy < initial_et_accuracy:\n",
    "    print(\"The accuracy of the new optimal model is lower than the initial model.\")\n",
    "else:\n",
    "    print(\"The accuracies of the new optimal model and the initial model are the same.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2db7982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Hyperparameters: {'n_estimators': 1000, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# to Define the hyperparameter grid\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'max_features': max_features\n",
    "}\n",
    "\n",
    "# To Create an ExtraTreesClassifier model\n",
    "et_classifier = ExtraTreesClassifier(random_state=1)\n",
    "\n",
    "# To Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    et_classifier,  # Estimator\n",
    "    param_distributions=hyperparameter_grid,\n",
    "    cv=5,\n",
    "    n_iter=10,  # Number of iterations for random search\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Verbosity level\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# To Fit the random search to the data\n",
    "random_search.fit(X_train_combined, y_train)\n",
    "\n",
    "# To Get the best hyperparameters\n",
    "best_hyperparameters = random_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b027aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the LGBM Classifier on the Test Set: 0.8090844570617459\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Lets Assume we have defined hyperparameters for the LGBM model\n",
    "lgbm_hyperparameters = {\n",
    "    'n_estimators': 100, \n",
    "    'max_depth': 6,      \n",
    "    'learning_rate': 0.1, \n",
    "    'random_state': 1\n",
    "}\n",
    "\n",
    "# Create and train the LGBM classifier\n",
    "lgbm_classifier = LGBMClassifier(**lgbm_hyperparameters)\n",
    "lgbm_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "lgbm_predictions = lgbm_classifier.predict(X_test_combined)\n",
    "\n",
    "# Calculate the accuracy of the LGBM model on the test set\n",
    "lgbm_accuracy = accuracy_score(y_test, lgbm_predictions)\n",
    "\n",
    "print(\"Accuracy of the LGBM Classifier on the Test Set:\", lgbm_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5b590a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the XGBoost Classifier on the Test Set: 0.8048261178140526\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# By Assuming we have defined hyperparameters for the XGBoost model\n",
    "xgb_hyperparameters = {\n",
    "    'n_estimators': 100, \n",
    "    'max_depth': 6,      \n",
    "    'learning_rate': 0.1, \n",
    "    'random_state': 1\n",
    "}\n",
    "\n",
    "# To Create and train the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(**xgb_hyperparameters)\n",
    "xgb_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# To Make predictions on the test set\n",
    "xgb_predictions = xgb_classifier.predict(X_test_combined)\n",
    "\n",
    "# To Calculate the accuracy of the XGBoost model on the test set\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "print(\"Accuracy of the XGBoost Classifier on the Test Set:\", xgb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8822858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest Classifier on the Test Set: 0.8140525195173882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# By Assuming we have defined hyperparameters for the Random Forest model\n",
    "rf_hyperparameters = {\n",
    "    'n_estimators': 100,  \n",
    "    'max_depth': 6,      \n",
    "    'min_samples_split': 2,  \n",
    "    'min_samples_leaf': 1,  \n",
    "    'max_features': 'auto',  \n",
    "    'random_state': 1\n",
    "}\n",
    "\n",
    "# To Create and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(**rf_hyperparameters)\n",
    "rf_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# To Make predictions on the test set\n",
    "rf_predictions = rf_classifier.predict(X_test_combined)\n",
    "\n",
    "# To Calculate the accuracy of the Random Forest model on the test set\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "\n",
    "print(\"Accuracy of the Random Forest Classifier on the Test Set:\", rf_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c040fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d55430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
